{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import jieba\n",
    "import re\n",
    "import os\n",
    "import time\n",
    "import gc\n",
    "\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import Adam, AdamW\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from transformers import BertTokenizer, BertModel, BertForSequenceClassification\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import f1_score, auc, roc_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import root_path, stopwords_path, train_query_path, train_reply_path, device, best_model_path, batch_size, \\\n",
    "    test_query_path, test_reply_path, user_dict_path, train_path, train_all_path, test_path, dev_path, pretrained_bert_path, lr, is_cuda, max_gradient_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_stopwords(file):\n",
    "    \"\"\"\n",
    "    加载停用词\n",
    "    :param file:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    with open(file, 'r', encoding='utf-8') as fp:\n",
    "        stopwords = fp.read().strip().splitlines()\n",
    "    return stopwords\n",
    "\n",
    "def filter_stopwords(sentence, stopwords):\n",
    "    \"\"\"\n",
    "    过滤停用词\n",
    "    :param sentence:\n",
    "    :param stopwords:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    text = ' '.join([word for word in sentence.split() if word not in stopwords])\n",
    "    return text\n",
    "\n",
    "def seg_sentence(sentence):\n",
    "    \"\"\"\n",
    "    分词\n",
    "    :param sentence:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    segeds = jieba.cut(sentence.strip())\n",
    "    return ' '.join(segeds)\n",
    "\n",
    "def filter_content(sentence):\n",
    "    \"\"\"\n",
    "    过滤特定内容\n",
    "    :param sentence:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # 替换数字，ccks数据中包含阿拉伯数字、中文数字\n",
    "    sentence = re.sub(r\"([1-9]\\d*\\.?\\d*)|(0\\.\\d*[1-9])\", \"数字x\", sentence)\n",
    "    sentence = re.sub(r\"([\\u96f6\\u4e00\\u4e8c\\u4e09\\u56db\\u4e94\\u516d\\u4e03\\u516b\\u4e5d\\u5341\\u767e\\u5343\\u4e07]+)\", \"数字x\", sentence)\n",
    "\n",
    "    return sentence\n",
    "\n",
    "def generate_dataset(query_path, reply_path, to_path, stopwords, dev_path=None, mode='train'):\n",
    "    \"\"\"\n",
    "    生成数据集\n",
    "    :param query_path:\n",
    "    :param reply_path:\n",
    "    :param to_path: 保存训练集/测试集的路径\n",
    "    :param mode:train / test / dev\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    if mode in ['train', 'dev']:\n",
    "        names = ['dialog_id', 'reply_id', 'reply_content', 'label']\n",
    "        encoding = 'utf-8'\n",
    "    else:\n",
    "        names = ['dialog_id', 'reply_id', 'reply_content']\n",
    "        encoding = 'gbk'\n",
    "\n",
    "    query_df = pd.read_csv(query_path, sep='\\t', encoding=encoding, names=['dialog_id', 'question'])\n",
    "\n",
    "    reply_df = pd.read_csv(reply_path, sep='\\t', encoding=encoding, names=names)\n",
    "    query_df = query_df.dropna()\n",
    "    reply_df = reply_df.dropna()\n",
    "\n",
    "    df = pd.merge(query_df, reply_df, how='inner')\n",
    "\n",
    "    for col in ['question', 'reply_content']:\n",
    "        # 过滤特殊文本\n",
    "        df[col] = df[col].apply(lambda x: filter_content(x))\n",
    "        # 分词\n",
    "        df[col] = df[col].apply(lambda x: seg_sentence(x))\n",
    "        # 过滤停用词（仅过滤标点符号）\n",
    "        df[col] = df[col].apply(lambda x: filter_stopwords(x, stopwords))\n",
    "        # 过滤停用词（仅过滤标点符号）后，会出现空字符串（原字符串仅有标点符号，过滤后就为空）的情况\n",
    "        df[col].replace(to_replace=r'^\\s*$', value=np.nan, inplace=True, regex=True)\n",
    "        df[col].fillna('符号x', inplace=True)\n",
    "    if mode == 'dev':\n",
    "        # 如果是dev，则将train分为训练集和验证集\n",
    "        train, dev = train_test_split(df, test_size=0.25)\n",
    "        train.to_csv(to_path, index=0)\n",
    "        dev.to_csv(dev_path, index=0)\n",
    "    else:\n",
    "        df.to_csv(to_path, index=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Dumping model to file cache /tmp/jieba.cache\n",
      "Loading model cost 0.925 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "jieba.load_userdict(user_dict_path)\n",
    "stopwords = load_stopwords(stopwords_path)\n",
    "generate_dataset(train_query_path, train_reply_path, train_all_path, stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QAMatchDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, mode):\n",
    "        assert mode in ['train', 'dev', 'test']\n",
    "\n",
    "        self.mode = mode\n",
    "        self.tokenizer = tokenizer\n",
    "        self.df = df\n",
    "        # self.df = pd.read_csv(file)\n",
    "        # self.seqs, self.seq_masks, self.seq_segments, self.labels = self.get_input(file)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        token_seq_1 = self.df.iloc[idx]['question']\n",
    "        token_seq_2 = self.df.iloc[idx]['reply_content']\n",
    "        if self.mode in ['train', 'dev']:\n",
    "            label_tensor = torch.tensor(self.df.iloc[idx]['label'])\n",
    "        else:\n",
    "            label_tensor = None\n",
    "        token_seq_1 = self.tokenizer.tokenize(token_seq_1)\n",
    "        token_seq_2 = self.tokenizer.tokenize(token_seq_2)\n",
    "\n",
    "        seq = [\"[CLS]\"] + token_seq_1 + [\"[SEP]\"] + token_seq_2 + [\"[SEP]\"]\n",
    "        seq = self.tokenizer.convert_tokens_to_ids(seq)\n",
    "\n",
    "        seq_segments = [0] * (len(token_seq_1) + 2) + [1] * (len(token_seq_2) + 1)\n",
    "\n",
    "        return torch.Tensor(seq).type(torch.long), torch.Tensor(seq_segments).type(torch.long), label_tensor\n",
    "\n",
    "    def collate_fn(self, samples):\n",
    "        seqs = [s[0] for s in samples]\n",
    "        seq_segments = [s[1] for s in samples]\n",
    "\n",
    "        if self.mode in ['train', 'dev']:\n",
    "            labels = torch.stack([s[2] for s in samples])\n",
    "        else:\n",
    "            labels = None\n",
    "\n",
    "        seqs = pad_sequence(seqs, batch_first=True)\n",
    "        seq_segments = pad_sequence(seq_segments, batch_first=True)\n",
    "\n",
    "        # attention mask处理\n",
    "        seq_masks = torch.zeros(seqs.shape, dtype=torch.long)\n",
    "        seq_masks = seq_masks.masked_fill(seqs != 0, 1)\n",
    "\n",
    "        return seqs, seq_masks, seq_segments, labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertModelTrain(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BertModelTrain, self).__init__()\n",
    "        self.bert = BertForSequenceClassification.from_pretrained(pretrained_bert_path, num_labels=2)\n",
    "        self.device = device\n",
    "        for param in self.bert.parameters():\n",
    "            param.requires_grad = True     # fine-tune，每个参数都要更新\n",
    "\n",
    "    def forward(self, batch_seqs, batch_seq_masks, batch_seq_segments, labels=None):\n",
    "        \"\"\"\n",
    "        :param batch_seqs: input_ids\n",
    "        :param batch_seq_masks: attention_mask\n",
    "        :param batch_seq_segments: token_type_ids\n",
    "        :param labels:\n",
    "        :return: outputs: (loss, logits, ...)\n",
    "                 outputs: (logits, ...)\n",
    "        \"\"\"\n",
    "        outputs = self.bert(input_ids=batch_seqs,\n",
    "                            attention_mask=batch_seq_masks,\n",
    "                            token_type_ids=batch_seq_segments,\n",
    "                            labels=labels)\n",
    "\n",
    "        if labels is not None:\n",
    "            logits = outputs[1]                   # shape: (batch, 2)\n",
    "            probabilities = nn.functional.softmax(logits, dim=-1)\n",
    "            # 将label:[0,1,1,0] -> y_true: tensor[[1,0], [0,1], [0,1], [1,0]], 即在每个label上的真实概率\n",
    "            y_true = torch.zeros(logits.shape)    # shape: (batch, 2)\n",
    "            for i in range(y_true.shape[0]):\n",
    "                y_true[i, labels[i]] = 1\n",
    "            loss = nn.functional.binary_cross_entropy_with_logits(logits.cpu(), y_true)\n",
    "            # loss = outputs[0]\n",
    "            outputs = (loss, ) + (logits,) + (probabilities, )\n",
    "        # probabilities = nn.functional.softmax(logits, dim=-1)\n",
    "        else:\n",
    "            logits = outputs[0]\n",
    "            probabilities = nn.functional.softmax(logits, dim=-1)\n",
    "            outputs = (logits, probabilities)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_dataloader, dev_dataloader, bert_tokenizer, best_model_path, fold, epochs=5, patience=3, checkpoint=None):\n",
    "    # ---------------------- Model definition ---------------------- #\n",
    "    print(\"\\t* Building model...\")\n",
    "    bulid_time = time.time()\n",
    "    model = BertModelTrain().to(device)\n",
    "    print(\"\\t* Building model time:{:.4f}s\".format(time.time()-bulid_time))\n",
    "    # ---------------------- Preparation for training -------------- #\n",
    "    param_optimizer = list(model.named_parameters())\n",
    "    # 这里，指定部分参数不参与权重衰减。\n",
    "    no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "    optimizer_grouped_parameters = [{\n",
    "        'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "        'weight_decay': 0.001\n",
    "    }, {\n",
    "        'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
    "        'weight_decay': 0.0\n",
    "    }]\n",
    "#     optimizer = AdamW(optimizer_grouped_parameters, lr=lr)\n",
    "    optimizer = Adam(model.parameters(), lr=lr)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.85, patience=patience)\n",
    "\n",
    "    best_score = 0.0    # 记录validation最好的结果\n",
    "    best_thres = 0.0\n",
    "    start_epoch = 1\n",
    "    # Data for loss curves plot\n",
    "    epoch_count = []\n",
    "    train_losses = []\n",
    "    valid_losses = []\n",
    "    best_model_saved_path = os.path.join(best_model_path, 'best-fine-tune-V1.4-k1.bin')\n",
    "\n",
    "    # 如果有给参数checkpoint，则继续训练\n",
    "    if checkpoint:\n",
    "        checkpoint = torch.load(checkpoint)\n",
    "        start_epoch = checkpoint['epoch'] + 1\n",
    "        best_score = checkpoint['best_score']\n",
    "        best_thres = checkpoint['best_thres']\n",
    "        print(\"\\t* Training will continue on existing model from epoch{}...\".format(start_epoch))\n",
    "        model.bert.load_state_dict(checkpoint['model'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "        epoch_count = checkpoint['epoch_count']\n",
    "        train_losses = checkpoint['train_losses']\n",
    "        valid_losses = checkpoint['valid_losses']\n",
    "\n",
    "    # Compute loss and accuracy before starting (or resuming) training\n",
    "    # 如果准备start training，这里的valid结果就是预训练BERT（做fine-tune之前）对下游任务的效果\n",
    "    # 如果准备resuming training，这里的valid结果就是上一次fine-tune的结果\n",
    "    valid_loss, valid_accuracy, valid_f1, valid_auc, thres = validate(model, dev_dataloader)\n",
    "    print(\"\\t* Validation loss before training: {:.4f}, accuracy:{:.4f}, \"\n",
    "          \"f1_score: {:.4f}, best_thres: {:.4f}, auc: {:.4f}\".\n",
    "          format(valid_loss, (valid_accuracy * 100), valid_f1, thres, valid_auc))\n",
    "    print(\"\\n\", 20 * \"=\", \"Training Bert model o device: {}\".format(device), 20 * \"=\")\n",
    "\n",
    "    patience_counter = 0\n",
    "    for epoch in range(start_epoch, epochs+1):\n",
    "        print(\"-> Start epoch {}\".format(epoch))\n",
    "        epoch_count.append(epoch)\n",
    "        # train\n",
    "        epoch_time, epoch_loss, epoch_accuracy, epoch_f1, epoch_auc = train_for_one_epoch(model,\n",
    "                                                                                          train_dataloader,\n",
    "                                                                                          optimizer,\n",
    "                                                                                          max_gradient_norm)\n",
    "        train_losses.append(epoch_loss)\n",
    "        print(\"-> Training time:{:.4f}s, loss: {:.4f}, accuracy: {:.4f}%, f1_score: {:.4f}, auc: {:.4f}\".\n",
    "              format(epoch_time, epoch_loss, epoch_accuracy*100, epoch_f1, epoch_auc))\n",
    "\n",
    "        # validation\n",
    "        valid_loss, valid_accuracy, valid_f1, valid_auc, thres = validate(model, dev_dataloader)\n",
    "        print(\"-> Validation loss: {:.4f}, accuracy: {:.4f}%, f1_score: {:.4f}, best_thres: {:.4f}, auc: {:.4f}\".\n",
    "              format(valid_loss, valid_accuracy * 100, valid_f1, thres, valid_auc))\n",
    "        valid_losses.append(valid_loss)\n",
    "        scheduler.step(valid_auc)\n",
    "\n",
    "        # 以valid_auc为评测标准\n",
    "        \n",
    "        if valid_auc < best_score:\n",
    "            patience_counter += 1\n",
    "        else:\n",
    "            best_score = valid_auc\n",
    "            best_thres = thres\n",
    "            patience_counter = 0\n",
    "            best_model_saved_path = os.path.join(best_model_path, 'best-fine-tune-V1.4-k'+str(fold)+'.bin')\n",
    "            torch.save({\n",
    "                \"epoch\": epoch,\n",
    "                \"model\": model.bert.state_dict(),\n",
    "                \"best_score\": best_score,\n",
    "                \"best_thres\": best_thres,\n",
    "                \"epochs_count\": epoch_count,\n",
    "                \"train_losses\": train_losses,\n",
    "                \"valid_losses\": valid_losses\n",
    "            }, best_model_saved_path)\n",
    "\n",
    "        if patience_counter >= patience:\n",
    "            print(\"-> Early stopping: patience limit reached, stopping...\")\n",
    "            break\n",
    "    if patience_counter != 0:\n",
    "        # 如果最后一个epoch不是最好的模型，则读取之前的最好的模型\n",
    "        best_checkpoint = torch.load(best_model_saved_path)\n",
    "        model.bert.load_state_dict(best_checkpoint['model'])\n",
    "    return model\n",
    "\n",
    "\n",
    "def train_for_one_epoch(model, dataloader, optimizer, max_gradient_norm):\n",
    "    model.train()\n",
    "\n",
    "    epoch_start_time = time.time()\n",
    "    running_loss = 0.0   # 记录整个epoch的累加loss\n",
    "    correct_count = 0.0\n",
    "    batch_avg_time = 0.0 # 记录该epoch平均batch花费时间\n",
    "    all_preds = []\n",
    "    all_pred_probas = []\n",
    "    all_labels = []\n",
    "\n",
    "    tqdm_dataloader = tqdm(dataloader)\n",
    "    for batch_index, data in enumerate(tqdm_dataloader):\n",
    "        batch_start_time = time.time()\n",
    "        if is_cuda:\n",
    "            data = [t.to(device) for t in data if t is not None]\n",
    "        # 梯度置零\n",
    "        optimizer.zero_grad()\n",
    "        seqs, seq_masks, seq_segments, labels = data\n",
    "        outputs = model(seqs, seq_masks, seq_segments, labels)\n",
    "        # 回传梯度\n",
    "        loss = outputs[0]\n",
    "        logits = outputs[1]\n",
    "        probabilities = outputs[2]\n",
    "        # probabilities = nn.functional.softmax(logits, dim=-1)\n",
    "        loss.backward()\n",
    "        # 梯度裁剪\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), max_gradient_norm)\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        _, pred = torch.max(logits, dim=1)\n",
    "        correct_count = correct_count + (pred == labels).sum().item()\n",
    "        batch_avg_time += time.time() - batch_start_time\n",
    "        # all_preds.append(pred)\n",
    "        all_labels.append(labels.cpu())\n",
    "        all_pred_probas.append(probabilities.detach().cpu())\n",
    "\n",
    "        description = \"Batch num: {}. Avg. batch proc. time: {:.4f}s, loss: {:.4f}\".\\\n",
    "            format(batch_index+1, batch_avg_time/(batch_index+1), running_loss/(batch_index+1))\n",
    "        tqdm_dataloader.set_description(description)\n",
    "        del data\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "    all_labels = torch.cat(all_labels)    # 把每个batch的labels平铺成一维tensor (samples, )\n",
    "    # all_preds = torch.cat(all_preds)      # 把每个batch的preds平铺成一维tensor (samples, )\n",
    "    all_pred_probas = torch.cat(all_pred_probas) # 把每个batch的probas平铺成tensor (samples, 2)\n",
    "\n",
    "    fpr, tpr, thresholds = roc_curve(all_labels, all_pred_probas[:, 1], pos_label=1)\n",
    "\n",
    "    epoch_loss = running_loss / len(dataloader)\n",
    "    epoch_accuracy = correct_count / len(dataloader.dataset)\n",
    "    epoch_time = time.time() - epoch_start_time\n",
    "    epoch_auc = auc(fpr, tpr)\n",
    "    # epoch_f1 = f1_score(all_labels, all_preds)\n",
    "\n",
    "    # return epoch_time, epoch_loss, epoch_accuracy, epoch_f1, epoch_auc\n",
    "    return epoch_time, epoch_loss, epoch_accuracy, 0, epoch_auc\n",
    "\n",
    "\n",
    "def validate(model, dataloader):\n",
    "    model.eval()\n",
    "    running_loss = 0.0  # 记录整个epoch的累加loss\n",
    "    correct_count = 0.0\n",
    "    # all_preds = []\n",
    "    all_labels = []\n",
    "    all_pred_probas = []\n",
    "    tqdm_dataloader = tqdm(dataloader)\n",
    "\n",
    "    # Deactivate autograd for evaluation\n",
    "    with torch.no_grad():   # 必须加这个，减少显存的使用\n",
    "        for batch_index, data in enumerate(tqdm_dataloader):\n",
    "            if is_cuda:\n",
    "                data = [t.to(device) for t in data if t is not None]\n",
    "\n",
    "            seqs, seq_masks, seq_segments, labels = data\n",
    "            outputs = model(seqs, seq_masks, seq_segments, labels)\n",
    "            loss = outputs[0]\n",
    "            logits = outputs[1]\n",
    "            probabilities = outputs[2]\n",
    "            # probabilities = nn.functional.softmax(logits, dim=-1)\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            # _, pred = torch.max(logits, dim=1)\n",
    "\n",
    "            # correct_count = correct_count + (pred == labels).sum().item()\n",
    "            # all_preds.append(pred.cpu())\n",
    "            all_labels.append(labels.cpu())\n",
    "            all_pred_probas.append(probabilities.cpu())\n",
    "            \n",
    "            del data\n",
    "            torch.cuda.empty_cache()\n",
    "                    \n",
    "    all_labels = torch.cat(all_labels)  # 把每个batch的labels平铺成一维tensor shape: (samples, )\n",
    "    # all_preds = torch.cat(all_preds)  # 把每个batch的preds平铺成一维tensor shape: (samples, )\n",
    "    all_pred_probas = torch.cat(all_pred_probas)  # 把每个batch的probas变成tensor（原来是[tensor, tensor, ...]）\n",
    "\n",
    "\n",
    "    best_f1, best_thres = search_f1(all_labels, all_pred_probas[:, 1])\n",
    "    all_preds = (all_pred_probas[:, 1] > best_thres).type(torch.long)\n",
    "    correct_count = (all_preds == all_labels).sum().item()\n",
    "\n",
    "    fpr, tpr, thresholds = roc_curve(all_labels, all_pred_probas[:, 1], pos_label=1)\n",
    "\n",
    "    valid_loss = running_loss / len(dataloader)\n",
    "    valid_acc = correct_count / len(dataloader.dataset)\n",
    "    # valid_f1 = f1_score(all_labels, all_preds)\n",
    "    valid_f1 = best_f1\n",
    "    valid_auc = auc(fpr, tpr)\n",
    "    return valid_loss, valid_acc, valid_f1, valid_auc, best_thres\n",
    "    # return valid_loss, valid_acc, 0, 0\n",
    "    \n",
    "def search_f1(y_true, y_pred):\n",
    "    \"\"\"\n",
    "\n",
    "    :param y_true: 一维tensor\n",
    "    :param y_pred: 一维tensor，y_pred[i]表示第i个样本在label为1上的预测概率\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    best_score = 0.0\n",
    "    best_thres = 0.0\n",
    "    for i in range(30, 60):\n",
    "        thres = i / 100\n",
    "        y_pred_bin = (y_pred > thres)   # 大于thres的为1，小于thres的为0\n",
    "        # print(\"y_pred_bin shape:\", y_pred_bin.shape)\n",
    "        score = f1_score(y_true, y_pred_bin)\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_thres = thres\n",
    "\n",
    "    return best_score, best_thres\n",
    "    \n",
    "def get_pred_probas(model, dataloader):\n",
    "    model.eval()\n",
    "    probas = None\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in dataloader:\n",
    "            # 将所有tensors移到GPU上\n",
    "            if is_cuda:\n",
    "                data = [t.to(device) for t in data if t is not None]\n",
    "\n",
    "            seqs, seq_masks, seq_segments = data[:3]\n",
    "            outputs = model(seqs,\n",
    "                            seq_masks,\n",
    "                            seq_segments)\n",
    "            logits = outputs[0]\n",
    "            probabilities = outputs[1]   # (batch, 2)\n",
    "            # probabilities = nn.functional.softmax(logits, dim=1)\n",
    "            # _, pred = torch.max(logits.data, dim=1)\n",
    "\n",
    "            if probas is None:\n",
    "                probas = probabilities\n",
    "            else:\n",
    "                # 将每个batch的预测结果拼接起来\n",
    "                probas = torch.cat([probas, probabilities])\n",
    "                \n",
    "            del data\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    return probas.cpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_fold_cross_val(train_df, test_df, k, bert_tokenizer, best_model_path, version):\n",
    "    kf = KFold(n_splits=5)\n",
    "    test_dataset = QAMatchDataset(test_df, bert_tokenizer, mode='test')\n",
    "    test_dataloader = DataLoader(test_dataset,\n",
    "                                 batch_size=batch_size,\n",
    "                                 num_workers=3,\n",
    "                                 collate_fn=test_dataset.collate_fn)\n",
    "    dev_labels = []\n",
    "    dev_probas = []\n",
    "    test_probas = []\n",
    "    for fold, (train_idxs, dev_idxs) in enumerate(kf.split(train_df)):\n",
    "        print(\"\\t* Start \"+str(fold)+\" fold\")\n",
    "        dev_labels.extend(train_df.iloc[dev_idxs]['label'].tolist())\n",
    "        # ---------------------- Data loading -------------------------- #\n",
    "        print(\"\\t* Building dataset...\")\n",
    "        train_dataset = QAMatchDataset(train_df.iloc[train_idxs], bert_tokenizer, 'train')\n",
    "        dev_dataset = QAMatchDataset(train_df.iloc[dev_idxs], bert_tokenizer, 'dev')\n",
    "\n",
    "        train_dataloader = DataLoader(train_dataset, batch_size=batch_size, num_workers=3,\n",
    "                                      collate_fn=train_dataset.collate_fn)\n",
    "        dev_dataloader = DataLoader(dev_dataset, batch_size=batch_size, num_workers=3,\n",
    "                                    collate_fn=dev_dataset.collate_fn)\n",
    "        best_model_fold_path = os.path.join(best_model_path, 'best-fine-tune-'+version+'-k'+str(fold)+'.bin')\n",
    "        if not(os.path.exists(best_model_fold_path)):\n",
    "            # 若已有\n",
    "            model = train(train_dataloader, dev_dataloader, bert_tokenizer, best_model_path, \n",
    "                          fold, epochs=5, patience=3, checkpoint=None)\n",
    "        else:\n",
    "            checkpoint = torch.load(best_model_fold_path)\n",
    "            model = BertModelTrain().to(device)\n",
    "            model.bert.load_state_dict(checkpoint['model'])\n",
    "        \n",
    "        dev_proba = get_pred_probas(model, dev_dataloader)\n",
    "        test_proba = get_pred_probas(model, test_dataloader)\n",
    "        \n",
    "        dev_probas.append(dev_proba)  # (k, len(dev_idxs), 2)\n",
    "        test_probas.append(test_proba) # (k, len(test_dataset), 2)\n",
    "#         model.to(torch.device('cpu'))\n",
    "        del model, train_dataloader, dev_dataloader, checkpoint\n",
    "        torch.cuda.empty_cache() \n",
    "        time.sleep(5)\n",
    "    \n",
    "    dev_probas = torch.cat(dev_probas)  # (len(train_df), 2)    # 把每一折的验证集的预测结果拼接，得到整个训练集的预测结果\n",
    "    f1, thres = search_f1(torch.tensor(dev_labels), dev_probas) # 找最好的F1和thres\n",
    "    \n",
    "    test_probas = torch.stack(test_probas) # (k, len(test_dataset), 2)， 只是把[tensor, tensor, ... ]转为tensor\n",
    "    test_probas = torch.mean(test_probas, dim=0)  # (len(test_dataset), 2)  取每一折的平均\n",
    "    test_preds = (test_probas[:, 1] > thres).type(torch.long)\n",
    "    return test_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calling BertTokenizer.from_pretrained() with the path to a single file or url is deprecated\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t* K fold training and validating...\n",
      "\t* Start 0 fold\n",
      "\t* Building dataset...\n",
      "\t* Start 1 fold\n",
      "\t* Building dataset...\n",
      "\t* Building model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/135 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t* Building model time:4.0103s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 135/135 [00:10<00:00, 12.86it/s]\n",
      "  0%|          | 0/540 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t* Validation loss before training: 0.6405, accuracy:42.9002, f1_score: 0.1520, best_thres: 0.3000, auc: 0.2824\n",
      "\n",
      " ==================== Training Bert model o device: cuda ====================\n",
      "-> Start epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch num: 7. Avg. batch proc. time: 0.2611s, loss: 0.6465:   1%|▏         | 7/540 [00:02<02:58,  2.98it/s]"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 76.00 MiB (GPU 0; 11.19 GiB total capacity; 9.34 GiB already allocated; 95.86 MiB free; 330.73 MiB cached)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-a7c4fb49e036>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mbert_tokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBertTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_bert_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'vocab.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\t* K fold training and validating...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mtest_preds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mk_fold_cross_val\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbert_tokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_model_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mversion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\t*Predicting...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mtest_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'pred'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_preds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-f934fea4cdd5>\u001b[0m in \u001b[0;36mk_fold_cross_val\u001b[0;34m(train_df, test_df, k, bert_tokenizer, best_model_path, version)\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0;31m# 若已有\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             model = train(train_dataloader, dev_dataloader, bert_tokenizer, best_model_path, \n\u001b[0;32m---> 27\u001b[0;31m                           fold, epochs=5, patience=3, checkpoint=None)\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0mcheckpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_model_fold_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-51d9c6088f29>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(train_dataloader, dev_dataloader, bert_tokenizer, best_model_path, fold, epochs, patience, checkpoint)\u001b[0m\n\u001b[1;32m     59\u001b[0m                                                                                           \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m                                                                                           \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                                                                                           max_gradient_norm)\n\u001b[0m\u001b[1;32m     62\u001b[0m         \u001b[0mtrain_losses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         print(\"-> Training time:{:.4f}s, loss: {:.4f}, accuracy: {:.4f}%, f1_score: {:.4f}, auc: {:.4f}\".\n",
      "\u001b[0;32m<ipython-input-5-51d9c6088f29>\u001b[0m in \u001b[0;36mtrain_for_one_epoch\u001b[0;34m(model, dataloader, optimizer, max_gradient_norm)\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0mseqs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_masks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_segments\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseqs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_masks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_segments\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m         \u001b[0;31m# 回传梯度\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-8ce436606e90>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, batch_seqs, batch_seq_masks, batch_seq_segments, labels)\u001b[0m\n\u001b[1;32m     19\u001b[0m                             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_seq_masks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m                             \u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_seq_segments\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m                             labels=labels)\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/transformers/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels)\u001b[0m\n\u001b[1;32m   1141\u001b[0m             \u001b[0mposition_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mposition_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1142\u001b[0m             \u001b[0mhead_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhead_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1143\u001b[0;31m             \u001b[0minputs_embeds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs_embeds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1144\u001b[0m         )\n\u001b[1;32m   1145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/transformers/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask)\u001b[0m\n\u001b[1;32m    732\u001b[0m             \u001b[0mhead_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhead_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    733\u001b[0m             \u001b[0mencoder_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoder_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 734\u001b[0;31m             \u001b[0mencoder_attention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoder_extended_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    735\u001b[0m         )\n\u001b[1;32m    736\u001b[0m         \u001b[0msequence_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/transformers/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask)\u001b[0m\n\u001b[1;32m    406\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    407\u001b[0m             layer_outputs = layer_module(\n\u001b[0;32m--> 408\u001b[0;31m                 \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhead_mask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_attention_mask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    409\u001b[0m             )\n\u001b[1;32m    410\u001b[0m             \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/transformers/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask)\u001b[0m\n\u001b[1;32m    378\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mcross_attention_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# add cross attentions if we output attention weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    379\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 380\u001b[0;31m         \u001b[0mintermediate_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintermediate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    381\u001b[0m         \u001b[0mlayer_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mintermediate_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlayer_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/transformers/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    330\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 332\u001b[0;31m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    333\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintermediate_act_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mweak_script_method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1406\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1407\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1408\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1409\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1410\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 76.00 MiB (GPU 0; 11.19 GiB total capacity; 9.34 GiB already allocated; 95.86 MiB free; 330.73 MiB cached)"
     ]
    }
   ],
   "source": [
    "version = 'V1.4'\n",
    "train_df = pd.read_csv(train_all_path)\n",
    "test_df = pd.read_csv(test_path)\n",
    "k = 5\n",
    "bert_tokenizer = BertTokenizer.from_pretrained(os.path.join(pretrained_bert_path, 'vocab.txt'))\n",
    "print(\"\\t* K fold training and validating...\")\n",
    "test_preds = k_fold_cross_val(train_df, test_df, k, bert_tokenizer, best_model_path, version)\n",
    "print(\"\\t*Predicting...\")\n",
    "test_df['pred'] = test_preds.cpu().numpy()\n",
    "print(\"\\t*Saving...\")\n",
    "time_str = '' + time.strftime(\"%Y%m%d%H%M\", time.localtime())\n",
    "test_df[['dialog_id', 'reply_id', 'pred']].to_csv(os.path.join(root_path,'submission/'+version+'_'+time_str+'.csv'),\n",
    "                                                  sep='\\t',\n",
    "                                                  index=0,\n",
    "                                                  header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
