{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 3254,
     "status": "ok",
     "timestamp": 1604647140528,
     "user": {
      "displayName": "梁寓杰",
      "photoUrl": "",
      "userId": "14609683328165856029"
     },
     "user_tz": -480
    },
    "id": "GUwDeFerT89A"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import jieba\n",
    "import re\n",
    "import os\n",
    "import time\n",
    "import gc\n",
    "\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import Adam, AdamW\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from transformers import BertTokenizer, BertModel, BertForSequenceClassification, BertConfig\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import f1_score, auc, roc_curve, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 2599,
     "status": "ok",
     "timestamp": 1604647140534,
     "user": {
      "displayName": "梁寓杰",
      "photoUrl": "",
      "userId": "14609683328165856029"
     },
     "user_tz": -480
    },
    "id": "m8PPhH8ICLaP",
    "outputId": "0d3ad4f2-8dd6-4543-b4c7-4722088f5703"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.11.0'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import transformers\n",
    "transformers.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 1950,
     "status": "ok",
     "timestamp": 1604647140539,
     "user": {
      "displayName": "梁寓杰",
      "photoUrl": "",
      "userId": "14609683328165856029"
     },
     "user_tz": -480
    },
    "id": "ZihHfS_LT64t"
   },
   "outputs": [],
   "source": [
    "from config import root_path, stopwords_path, device, best_model_path, batch_size, train_V0_path, test_V0_path,\\\n",
    "    user_dict_path, train_augmented_V0201_path, train_augmented_V0204_path, train_all_path, test_path, pretrained_bert_path, lr, \\\n",
    "    is_cuda, max_gradient_norm, num_directions, lstm_hidden_size\n",
    "import config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N62ZZGeoT644"
   },
   "source": [
    "## dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 979,
     "status": "ok",
     "timestamp": 1604647146335,
     "user": {
      "displayName": "梁寓杰",
      "photoUrl": "",
      "userId": "14609683328165856029"
     },
     "user_tz": -480
    },
    "id": "goCLd4xXT644"
   },
   "outputs": [],
   "source": [
    "class QAMatchDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, max_seq_len, mode):\n",
    "        assert mode in ['train', 'dev', 'test']\n",
    "\n",
    "        self.mode = mode\n",
    "        self.tokenizer = tokenizer\n",
    "        self.df = df\n",
    "        self.max_seq_len = max_seq_len\n",
    "        # self.df = pd.read_csv(file)\n",
    "        # self.seqs, self.seq_masks, self.seq_segments, self.labels = self.get_input(file)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        token_seq_1 = self.df.iloc[idx]['question']\n",
    "        token_seq_2 = self.df.iloc[idx]['reply_content']\n",
    "        if self.mode in ['train', 'dev']:\n",
    "            label_tensor = torch.tensor(self.df.iloc[idx]['label'])\n",
    "        else:\n",
    "            label_tensor = None\n",
    "        token_seq_1 = self.tokenizer.tokenize(token_seq_1)\n",
    "        token_seq_2 = self.tokenizer.tokenize(token_seq_2)\n",
    "#         print(\"token_seq_1:\", token_seq_1, \"lens:\", len(token_seq_1))|\n",
    "#         print(\"token_seq_2:\", token_seq_2, \"lens:\", len(token_seq_2))\n",
    "        \n",
    "        # truncate\n",
    "        if len(token_seq_1) > self.max_seq_len:\n",
    "            token_seq_1 = token_seq_1[:self.max_seq_len]\n",
    "        if len(token_seq_2) > self.max_seq_len:\n",
    "            token_seq_2 = token_seq_2[:self.max_seq_len]\n",
    "        \n",
    "        # padding\n",
    "        token_seq_1 += ['[PAD]'] * (self.max_seq_len - len(token_seq_1))\n",
    "        token_seq_2 += ['[PAD]'] * (self.max_seq_len - len(token_seq_2))\n",
    "        \n",
    "        seq = [\"[CLS]\"] + token_seq_1 + [\"[SEP]\"] + token_seq_2 + [\"[SEP]\"]\n",
    "        seq = self.tokenizer.convert_tokens_to_ids(seq)\n",
    "\n",
    "        seq_segment = [0] * (len(token_seq_1) + 2) + [1] * (len(token_seq_2) + 1)\n",
    "        \n",
    "        return torch.Tensor(seq).type(torch.long), torch.Tensor(seq_segment).type(torch.long), label_tensor\n",
    "    \n",
    "    def collate_fn(self, samples):\n",
    "#         print(samples[0])\n",
    "        seqs = torch.stack([s[0] for s in samples])\n",
    "        seq_segments = torch.stack([s[1] for s in samples])\n",
    "\n",
    "        if self.mode in ['train', 'dev']:\n",
    "            labels = torch.stack([s[2] for s in samples])\n",
    "        else:\n",
    "            labels = None\n",
    "\n",
    "        # attention mask处理\n",
    "        seq_masks = torch.zeros(seqs.shape, dtype=torch.long)\n",
    "        seq_masks = seq_masks.masked_fill(seqs != 0, 1)\n",
    "\n",
    "        return seqs, seq_masks, seq_segments, labels\n",
    "    def __len__(self):\n",
    "        return len(self.df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TcQmyLC8T647"
   },
   "source": [
    "## model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 768,
     "status": "ok",
     "timestamp": 1604647147355,
     "user": {
      "displayName": "梁寓杰",
      "photoUrl": "",
      "userId": "14609683328165856029"
     },
     "user_tz": -480
    },
    "id": "YInuSpqWT648"
   },
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    # 这个模型的作用：接受一个句子的embedding（从pretrain model中提取的），输出这个句子的句向量\n",
    "    # 最后输出的句向量大小：len(window_sizes) * feature_size ,即窗口个数 * 由每个窗口提取到的特征\n",
    "    # 积后得到的feature_map的大小 = (seq_len - window_size) / stride + 1\n",
    "    def __init__(self, embedding_size, feature_size, window_sizes, max_seq_len):\n",
    "        super(CNN, self).__init__()\n",
    "        self.convs = nn.ModuleList([\n",
    "            nn.Sequential(nn.Conv1d(in_channels=embedding_size, out_channels=feature_size, kernel_size=h),\n",
    "                         nn.ReLU(),\n",
    "                         nn.MaxPool1d(kernel_size=max_seq_len-h+1))   \n",
    "            for h in window_sizes\n",
    "        ])\n",
    "        # Conv1d指的就是在纵列方向上做卷积，out_channels指每种kernel(窗口)要有几个\n",
    "        # MaxPool1d指的就是在纵列方向上做池化，kernel_size设成feature_map的大小，就相当于对每个feature_map做max pool\n",
    "        # 以下是网络结构中每一层的维度变化\n",
    "#                 x                             permute                                conv                             MaxPool1d\n",
    "# (batch, seq_len, embedding_size) -> (batch, embedding_size, seq_len) -> (batch, feature_size, max_seq_len-h+1) -> (batch, feature_size, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        :param x: (batch, max_seq_len, embedding_size)\n",
    "        \n",
    "        :return output: (batch, len(window_sizes) * feature_size)\n",
    "        \"\"\"\n",
    "        x = x.permute(0, 2, 1)   # (batch, embedding_size, seq_len)  因为一维卷积是在最后维度上扫的\n",
    "        out = [conv(x) for conv in self.convs]    # out[i]: (batch, feature_size, 1)\n",
    "        out = torch.cat(out, dim=1)    # (batch, len(window_sizes)*feature_size, 1) 把所有窗口得到的feature拼接起来\n",
    "        out = out.view(-1, out.size(1)) # (batch, len(window_sizes)*feature_size)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "class BertModelWithCNN(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(BertModelWithCNN, self).__init__()        \n",
    "        self.bert_config = BertConfig.from_pretrained(os.path.join(config.pretrained_bert_path, 'config.json'))\n",
    "#         self.bert_config.output_hidden_states = True\n",
    "        self.max_seq_len = config.max_seq_len_r\n",
    "        self.bert = BertModel.from_pretrained(config.pretrained_bert_path, output_hidden_states=False)\n",
    "        \n",
    "        # 用max_seq_len_r作为q和r的最大序列长度\n",
    "        self.cnn = CNN(self.bert_config.hidden_size, config.feature_size, config.window_sizes, self.max_seq_len)\n",
    "        \n",
    "        self.dropout = nn.Dropout(p=config.dropout_rate)\n",
    "        self.linear = nn.Linear(2 * self.bert_config.hidden_size + 4 * len(config.window_sizes) * config.feature_size, 1)\n",
    "        self.loss_fn = nn.BCELoss()\n",
    "        for param in self.bert.parameters():\n",
    "            param.requires_grad = True     # fine-tune，每个参数都要更新\n",
    "\n",
    "    def forward(self, batch_seqs, batch_seq_masks, batch_seq_segments, labels=None):\n",
    "        \"\"\"\n",
    "        :param batch_seqs: input_ids\n",
    "        :param batch_seq_masks: attention_mask\n",
    "        :param batch_seq_segments: token_type_ids\n",
    "        :param labels:\n",
    "        :return: outputs: (loss, logits, ...)\n",
    "                 outputs: (logits, ...)\n",
    "        注：\n",
    "        hidden_size: bert中的hidden_size\n",
    "        lstm_hidden_szie: lstm中的hidden_size\n",
    "        \"\"\"\n",
    "        # last_hidden_state, (batch_size, sequence_length, hidden_size)\n",
    "        last_hidden_state, pooler_output = self.bert(input_ids=batch_seqs,\n",
    "                                                    attention_mask=batch_seq_masks,\n",
    "                                                    token_type_ids=batch_seq_segments)[:2]\n",
    "        \n",
    "        last_cls_hidden_state = last_hidden_state[:, 0]\n",
    "        \n",
    "        q_embeddings = last_hidden_state[:, 1 : self.max_seq_len+2]    # 第一句+[SEP]\n",
    "        r_embeddings = last_hidden_state[:, self.max_seq_len+2 :]   # 第二句+[SEP]\n",
    "        \n",
    "        q_cnn_embeddings = self.cnn(q_embeddings)   # (batch, len(window_sizes)*feature_size)\n",
    "        r_cnn_embeddings = self.cnn(r_embeddings)   # (batch, len(window_sizes)*feature_size)\n",
    "        \n",
    "        q_r_cnn_gap = torch.abs(q_cnn_embeddings - r_cnn_embeddings)\n",
    "        q_r_cnn_muliple = q_cnn_embeddings * r_cnn_embeddings\n",
    "        \n",
    "        # concatenate these four tensor -> (batch_size, 2 * hidden_size 4 * len(window_sizes) * feature_size)\n",
    "        x = torch.cat([pooler_output, last_cls_hidden_state, q_cnn_embeddings, r_cnn_embeddings, q_r_cnn_gap, q_r_cnn_muliple], dim=1)\n",
    "\n",
    "        # dropout\n",
    "        x = self.dropout(x)\n",
    "        # FC层 -> (batch, 1)\n",
    "        x = self.linear(x)\n",
    "        # sigmoid\n",
    "        output = torch.sigmoid(x)    # (batch_size, 1) 即模型预测每个样本为1的概率\n",
    "\n",
    "        logits = x\n",
    "        proba_0 = 1.0 - output     # (batch_size, 1)\n",
    "        probabilities = torch.cat((proba_0, output), dim=1)   # (batch_size, 2)\n",
    "        if labels is not None:\n",
    "            # 有标签，则返回loss, logits, probabilities\n",
    "            loss = self.loss_fn(output.squeeze(), labels.type(torch.float))\n",
    "            outputs = (loss, logits, probabilities)\n",
    "        else:\n",
    "            # 无标签，则返回logits, probabilities\n",
    "            outputs = (logits, probabilities)\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def writeToLog(path, content):\n",
    "    with open(path, 'a') as fp:\n",
    "        fp.write(content)\n",
    "        fp.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vCGlq6qBT64_"
   },
   "source": [
    "## train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 1501,
     "status": "ok",
     "timestamp": 1604647152153,
     "user": {
      "displayName": "梁寓杰",
      "photoUrl": "",
      "userId": "14609683328165856029"
     },
     "user_tz": -480
    },
    "id": "feiElGnzT65B"
   },
   "outputs": [],
   "source": [
    "def train(train_dataloader, dev_dataloader, bert_tokenizer, best_model_path, output_path, fold, version, epochs=5, patience=2, checkpoint=None):\n",
    "    # ---------------------- Model definition ---------------------- #\n",
    "    print(\"\\t* Building model...\")\n",
    "    bulid_time = time.time()\n",
    "    model = BertModelWithCNN(config).to(device)\n",
    "    print(\"\\t* Building model time:{:.4f}s\".format(time.time()-bulid_time))\n",
    "    # ---------------------- Preparation for training -------------- #\n",
    "    param_optimizer = list(model.named_parameters())\n",
    "    # 这里，指定部分参数不参与权重衰减\n",
    "    no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "    optimizer_grouped_parameters = [{\n",
    "        'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "        'weight_decay': 0.001\n",
    "    }, {\n",
    "        'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
    "        'weight_decay': 0.0\n",
    "    }]\n",
    "    # optimizer = AdamW(optimizer_grouped_parameters, lr=lr)\n",
    "    optimizer = Adam(model.parameters(), lr=lr)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.85, patience=patience)\n",
    "\n",
    "    best_score = 0.0    # 记录validation最好的结果\n",
    "    best_thres = 0.0\n",
    "    start_epoch = 1\n",
    "    # Data for loss curves plot\n",
    "    epoch_count = []\n",
    "    train_losses = []\n",
    "    valid_losses = []\n",
    "    best_model_saved_path = os.path.join(best_model_path, 'best-fine-tune-'+version+'-k'+str(fold)+'.bin')\n",
    "\n",
    "    # 如果有给参数checkpoint，则继续训练\n",
    "    if checkpoint:\n",
    "        checkpoint = torch.load(checkpoint)\n",
    "        start_epoch = checkpoint['epoch'] + 1\n",
    "        best_score = checkpoint['best_score']\n",
    "        best_thres = checkpoint['best_thres']\n",
    "        print(\"\\t* Training will continue on existing model from epoch{}...\".format(start_epoch))\n",
    "        model.load_state_dict(checkpoint['model'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "        epoch_count = checkpoint['epoch_count']\n",
    "        train_losses = checkpoint['train_losses']\n",
    "        valid_losses = checkpoint['valid_losses']\n",
    "\n",
    "    # Compute loss and accuracy before starting (or resuming) training\n",
    "    # 如果准备start training，这里的valid结果就是预训练BERT（做fine-tune之前）对下游任务的效果\n",
    "    # 如果准备resuming training，这里的valid结果就是上一次fine-tune的结果\n",
    "    valid_loss, valid_accuracy, valid_f1, valid_auc, thres = validate(model, dev_dataloader)\n",
    "    print(\"\\t* Validation loss before training: {:.4f}, accuracy:{:.4f}, \"\n",
    "          \"f1_score: {:.4f}, best_thres: {:.4f}, auc: {:.4f}\".\n",
    "          format(valid_loss, (valid_accuracy * 100), valid_f1, thres, valid_auc))\n",
    "    print(\"\\n\", 20 * \"=\", \"Training Bert model o device: {}\".format(device), 20 * \"=\")\n",
    "\n",
    "    patience_counter = 0\n",
    "    for epoch in range(start_epoch, epochs+1):\n",
    "        print(\"-> Start epoch {}\".format(epoch))\n",
    "        writeToLog(output_path, \"-> Start epoch {}\".format(epoch))\n",
    "        epoch_count.append(epoch)\n",
    "        # train\n",
    "        epoch_time, epoch_loss, epoch_accuracy, epoch_f1, epoch_auc = train_for_one_epoch(model,\n",
    "                                                                                          train_dataloader,\n",
    "                                                                                          optimizer,\n",
    "                                                                                          max_gradient_norm)\n",
    "        train_losses.append(epoch_loss)\n",
    "        print(\"-> Training time:{:.4f}s, loss: {:.4f}, accuracy: {:.4f}%, f1_score: {:.4f}, auc: {:.4f}\".\n",
    "              format(epoch_time, epoch_loss, epoch_accuracy*100, epoch_f1, epoch_auc))\n",
    "        writeToLog(output_path, \"-> Training time:{:.4f}s, loss: {:.4f}, accuracy: {:.4f}%, f1_score: {:.4f}, auc: {:.4f}\".\n",
    "              format(epoch_time, epoch_loss, epoch_accuracy*100, epoch_f1, epoch_auc))\n",
    "        \n",
    "        # validation\n",
    "        valid_loss, valid_accuracy, valid_f1, valid_auc, thres = validate(model, dev_dataloader)\n",
    "        print(\"-> Validation loss: {:.4f}, accuracy: {:.4f}%, f1_score: {:.4f}, best_thres: {:.4f}, auc: {:.4f}\".\n",
    "              format(valid_loss, valid_accuracy * 100, valid_f1, thres, valid_auc))\n",
    "        writeToLog(output_path, \"-> Validation loss: {:.4f}, accuracy: {:.4f}%, f1_score: {:.4f}, best_thres: {:.4f}, auc: {:.4f}\".\n",
    "              format(valid_loss, valid_accuracy * 100, valid_f1, thres, valid_auc))\n",
    "        \n",
    "        valid_losses.append(valid_loss)\n",
    "        scheduler.step(valid_auc)\n",
    "\n",
    "        # 以valid_auc为评测标准\n",
    "        \n",
    "        if valid_auc <= best_score:\n",
    "            patience_counter += 1\n",
    "        else:\n",
    "            best_score = valid_auc\n",
    "            best_thres = thres\n",
    "            patience_counter = 0\n",
    "            best_model_saved_path = os.path.join(best_model_path, 'best-fine-tune-'+version+'-k'+str(fold)+'.bin')\n",
    "            torch.save({\n",
    "                \"epoch\": epoch,\n",
    "                \"model\": model.state_dict(),\n",
    "                \"best_score\": best_score,\n",
    "                \"best_thres\": best_thres,\n",
    "                \"epochs_count\": epoch_count,\n",
    "                \"train_losses\": train_losses,\n",
    "                \"valid_losses\": valid_losses\n",
    "            }, best_model_saved_path)\n",
    "\n",
    "        if patience_counter >= patience:\n",
    "            print(\"-> Early stopping: patience limit reached, stopping...\")\n",
    "            break\n",
    "    if patience_counter != 0:\n",
    "        # 如果最后一个epoch不是最好的模型，则读取之前的最好的模型\n",
    "        best_checkpoint = torch.load(best_model_saved_path)\n",
    "        model.load_state_dict(best_checkpoint['model'])\n",
    "    return model, best_score\n",
    "\n",
    "\n",
    "def train_for_one_epoch(model, dataloader, optimizer, max_gradient_norm):\n",
    "    model.train()\n",
    "\n",
    "    epoch_start_time = time.time()\n",
    "    running_loss = 0.0   # 记录整个epoch的累加loss\n",
    "    correct_count = 0.0\n",
    "    batch_avg_time = 0.0 # 记录该epoch平均batch花费时间\n",
    "    all_preds = []\n",
    "    all_pred_probas = []\n",
    "    all_labels = []\n",
    "\n",
    "    tqdm_dataloader = tqdm(dataloader)\n",
    "    for batch_index, data in enumerate(tqdm_dataloader):\n",
    "        batch_start_time = time.time()\n",
    "        if is_cuda:\n",
    "            data = [t.to(device) for t in data if t is not None]\n",
    "        # 梯度置零\n",
    "        optimizer.zero_grad()\n",
    "        seqs, seq_masks, seq_segments, labels = data\n",
    "        outputs = model(seqs, seq_masks, seq_segments, labels)\n",
    "        # 回传梯度\n",
    "        loss = outputs[0]\n",
    "        logits = outputs[1]\n",
    "        probabilities = outputs[2]\n",
    "        # probabilities = nn.functional.softmax(logits, dim=-1)\n",
    "        loss.backward()\n",
    "        # 梯度裁剪\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), max_gradient_norm)\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        pred = torch.argmax(probabilities, dim=1)\n",
    "        correct_count = correct_count + (pred == labels).sum().item()\n",
    "        batch_avg_time += time.time() - batch_start_time\n",
    "        all_preds.append(pred.cpu())\n",
    "        all_labels.append(labels.cpu())\n",
    "        all_pred_probas.append(probabilities.detach().cpu())\n",
    "\n",
    "        description = \"Batch num: {}. Avg. batch proc. time: {:.4f}s, loss: {:.4f}\".\\\n",
    "            format(batch_index+1, batch_avg_time/(batch_index+1), running_loss/(batch_index+1))\n",
    "        tqdm_dataloader.set_description(description)\n",
    "        del data\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "    all_labels = torch.cat(all_labels)    # 把每个batch的labels平铺成一维tensor (samples, )\n",
    "    all_preds = torch.cat(all_preds)      # 把每个batch的preds平铺成一维tensor (samples, )\n",
    "    all_pred_probas = torch.cat(all_pred_probas) # 把每个batch的probas平铺成tensor (samples, 2)\n",
    "\n",
    "    fpr, tpr, thresholds = roc_curve(all_labels, all_pred_probas[:, 1], pos_label=1)\n",
    "\n",
    "    epoch_loss = running_loss / len(dataloader)\n",
    "    epoch_accuracy = correct_count / len(dataloader.dataset)\n",
    "    epoch_time = time.time() - epoch_start_time\n",
    "    epoch_auc = auc(fpr, tpr)\n",
    "    epoch_f1 = f1_score(all_labels, all_preds)\n",
    "\n",
    "    return epoch_time, epoch_loss, epoch_accuracy, epoch_f1, epoch_auc\n",
    "#     return epoch_time, epoch_loss, epoch_accuracy, 0, epoch_auc\n",
    "\n",
    "\n",
    "def validate(model, dataloader):\n",
    "    model.eval()\n",
    "    running_loss = 0.0  # 记录整个epoch的累加loss\n",
    "    correct_count = 0.0\n",
    "    # all_preds = []\n",
    "    all_labels = []\n",
    "    all_pred_probas = []\n",
    "    tqdm_dataloader = tqdm(dataloader)\n",
    "\n",
    "    # Deactivate autograd for evaluation\n",
    "    with torch.no_grad():   # 必须加这个，减少显存的使用\n",
    "        for batch_index, data in enumerate(tqdm_dataloader):\n",
    "            if is_cuda:\n",
    "                data = [t.to(device) for t in data if t is not None]\n",
    "\n",
    "            seqs, seq_masks, seq_segments, labels = data\n",
    "            outputs = model(seqs, seq_masks, seq_segments, labels)\n",
    "            loss = outputs[0]\n",
    "            logits = outputs[1]\n",
    "            probabilities = outputs[2]\n",
    "            # probabilities = nn.functional.softmax(logits, dim=-1)\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            # _, pred = torch.max(logits, dim=1)\n",
    "\n",
    "            # correct_count = correct_count + (pred == labels).sum().item()\n",
    "            # all_preds.append(pred.cpu())\n",
    "            all_labels.append(labels.cpu())\n",
    "            all_pred_probas.append(probabilities.cpu())\n",
    "            \n",
    "            del data\n",
    "            torch.cuda.empty_cache()\n",
    "                    \n",
    "    all_labels = torch.cat(all_labels)  # 把每个batch的labels平铺成一维tensor shape: (samples, )\n",
    "    # all_preds = torch.cat(all_preds)  # 把每个batch的preds平铺成一维tensor shape: (samples, )\n",
    "    all_pred_probas = torch.cat(all_pred_probas)  # 把每个batch的probas变成tensor（原来是[tensor, tensor, ...]）\n",
    "\n",
    "\n",
    "    # best_f1, best_thres = search_f1(all_labels, all_pred_probas[:, 1])\n",
    "    # all_preds = (all_pred_probas[:, 1] > best_thres).type(torch.long)\n",
    "    all_preds = torch.argmax(all_pred_probas, dim=1)\n",
    "    correct_count = (all_preds == all_labels).sum().item()\n",
    "\n",
    "    fpr, tpr, thresholds = roc_curve(all_labels, all_pred_probas[:, 1], pos_label=1)\n",
    "\n",
    "    valid_loss = running_loss / len(dataloader)\n",
    "    valid_acc = correct_count / len(dataloader.dataset)\n",
    "    valid_f1 = f1_score(all_labels, all_preds)\n",
    "    # valid_f1 = best_f1\n",
    "    valid_auc = auc(fpr, tpr)\n",
    "    best_thres = 0\n",
    "    return valid_loss, valid_acc, valid_f1, valid_auc, best_thres\n",
    "    # return valid_loss, valid_acc, 0, 0\n",
    "    \n",
    "def search_f1(y_true, y_pred):\n",
    "    \"\"\"\n",
    "\n",
    "    :param y_true: 一维tensor\n",
    "    :param y_pred: 一维tensor，y_pred[i]表示第i个样本在label为1上的预测概率\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    best_score = 0.0\n",
    "    best_thres = 0.0\n",
    "    for i in range(30, 70):\n",
    "        thres = i / 100\n",
    "        y_pred_bin = (y_pred > thres)   # 大于thres的为1，小于thres的为0\n",
    "        # print(\"y_pred_bin shape:\", y_pred_bin.shape)\n",
    "        score = f1_score(y_true, y_pred_bin)\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_thres = thres\n",
    "\n",
    "    return best_score, best_thres\n",
    "    \n",
    "def get_pred_probas(model, dataloader):\n",
    "    model.eval()\n",
    "    probas = None\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in dataloader:\n",
    "            # 将所有tensors移到GPU上\n",
    "            if is_cuda:\n",
    "                data = [t.to(device) for t in data if t is not None]\n",
    "#             print(len(data))\n",
    "#             print(data)\n",
    "#             break\n",
    "            seqs, seq_masks, seq_segments = data[:3]\n",
    "            outputs = model(seqs,\n",
    "                            seq_masks,\n",
    "                            seq_segments)\n",
    "            logits = outputs[0]\n",
    "            probabilities = outputs[1]   # (batch, 2)\n",
    "            # probabilities = nn.functional.softmax(logits, dim=1)\n",
    "            # _, pred = torch.max(logits.data, dim=1)\n",
    "\n",
    "            if probas is None:\n",
    "                probas = probabilities\n",
    "            else:\n",
    "                # 将每个batch的预测结果拼接起来\n",
    "                probas = torch.cat([probas, probabilities])\n",
    "                \n",
    "            del data\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    return probas.cpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hPlXyVxaT65G"
   },
   "source": [
    "## KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 649,
     "status": "ok",
     "timestamp": 1604647152686,
     "user": {
      "displayName": "梁寓杰",
      "photoUrl": "",
      "userId": "14609683328165856029"
     },
     "user_tz": -480
    },
    "id": "bPHMPZyOT65H"
   },
   "outputs": [],
   "source": [
    "def k_fold_cross_val(train_df, test_df, k, bert_tokenizer, best_model_path, output_path, version):\n",
    "    kf = KFold(n_splits=k)\n",
    "    test_dataset = QAMatchDataset(test_df, bert_tokenizer, config.max_seq_len_r, mode='test')\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=batch_size, num_workers=3, collate_fn=test_dataset.collate_fn)\n",
    "    dev_labels = []\n",
    "    dev_probas = []\n",
    "    k_test_probas = []\n",
    "    k_best_scores = []\n",
    "    for fold, (train_idxs, dev_idxs) in enumerate(kf.split(train_df)):\n",
    "        print(\"\\t* Start \"+str(fold)+\" fold\")\n",
    "        writeToLog(output_path, \"\\t* Start \"+str(fold)+\" fold\")\n",
    "        dev_labels.extend(train_df.iloc[dev_idxs]['label'].tolist())\n",
    "        # ---------------------- Data loading -------------------------- #\n",
    "        print(\"\\t* Building dataset...\")\n",
    "        train_dataset = QAMatchDataset(train_df.iloc[train_idxs], bert_tokenizer, config.max_seq_len_r, 'train')\n",
    "        dev_dataset = QAMatchDataset(train_df.iloc[dev_idxs], bert_tokenizer, config.max_seq_len_r, 'dev')\n",
    "\n",
    "        train_dataloader = DataLoader(train_dataset, batch_size=batch_size, num_workers=3,\n",
    "                                      collate_fn=train_dataset.collate_fn)\n",
    "        dev_dataloader = DataLoader(dev_dataset, batch_size=batch_size, num_workers=3,\n",
    "                                    collate_fn=dev_dataset.collate_fn)\n",
    "        best_model_fold_path = os.path.join(best_model_path, 'best-fine-tune-'+version+'-k'+str(fold)+'.bin')\n",
    "        checkpoint = None\n",
    "        if not(os.path.exists(best_model_fold_path)):\n",
    "            # 若没有\n",
    "            model, best_score = train(train_dataloader, dev_dataloader, bert_tokenizer, best_model_path, output_path, \n",
    "                                      fold, version, epochs=5, patience=3, checkpoint=None)\n",
    "        else:\n",
    "            checkpoint = torch.load(best_model_fold_path)\n",
    "            model = BertModelWithCNN(config).to(device)\n",
    "            model.load_state_dict(checkpoint['model'])\n",
    "            best_score = checkpoint['best_score']\n",
    "        k_best_scores.append(best_score)\n",
    "        \n",
    "        fold_dev_proba = get_pred_probas(model, dev_dataloader)\n",
    "        fold_test_proba = get_pred_probas(model, test_dataloader)\n",
    "        \n",
    "        dev_probas.append(fold_dev_proba)  # (k, len(dev_idxs), 2)\n",
    "        k_test_probas.append(fold_test_proba) # (k, len(test_dataset), 2)\n",
    "#         model.to(torch.device('cpu'))\n",
    "        del model, train_dataloader, dev_dataloader, checkpoint\n",
    "        torch.cuda.empty_cache() \n",
    "        time.sleep(5)\n",
    "    \n",
    "    dev_probas = torch.cat(dev_probas)  # (len(train_df), 2)    # 把每一折的验证集的预测结果拼接，得到整个训练集的预测结果\n",
    "    \n",
    "    k_test_probas = torch.stack(k_test_probas) # (k, len(test_dataset), 2)， 只是把[tensor, tensor, ... ]转为tensor\n",
    "#     test_probas = torch.mean(k_test_probas, dim=0)  # (len(test_dataset), 2)  取每一折的平均\n",
    "\n",
    "    # k折模型加权融合\n",
    "    k_best_scores = np.array(k_best_scores)              \n",
    "    k_weights = k_best_scores / k_best_scores.sum()             # (k,)\n",
    "    k_weights = np.expand_dims(np.expand_dims(k_weights,1),1)   # (k, 1, 1)\n",
    "    print('k_best_score :', k_best_scores)\n",
    "    print('k weights :', k_weights)\n",
    "    k_test_probas = k_test_probas * k_weights               # 广播机制，使得每个模型预测的概率乘上该模型的权重 (k, len(test_dataset), 2)\n",
    "    test_probas = torch.sum(k_test_probas, dim=0)           # 求和\n",
    "    # search f1\n",
    "    best_f1, best_thres = search_f1(dev_labels, dev_probas[:, 1])\n",
    "    print(best_f1, best_thres)\n",
    "    test_preds = (test_probas[:, 1] > best_thres).type(torch.long)\n",
    "    \n",
    "    # 不用search f1\n",
    "    # test_preds = torch.argmax(test_probas, dim=1) \n",
    "    return test_preds, k_test_probas, dev_probas, dev_labels, best_f1, best_thres"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8Kf4-Fr6T65Y"
   },
   "source": [
    "## 操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qs0l9xpYT65Y",
    "outputId": "817beebe-3e63-4b30-ccc9-1b73ee5458f7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calling BertTokenizer.from_pretrained() with the path to a single file or url is deprecated\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t* K fold training and validating...\n",
      "\t* Start 0 fold\n",
      "\t* Building dataset...\n",
      "\t* Building model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/270 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t* Building model time:5.6000s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 270/270 [00:21<00:00, 12.64it/s]\n",
      "  0%|          | 0/1080 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t* Validation loss before training: 0.6978, accuracy:49.6410, f1_score: 0.2835, best_thres: 0.0000, auc: 0.4548\n",
      "\n",
      " ==================== Training Bert model o device: cuda ====================\n",
      "-> Start epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch num: 1080. Avg. batch proc. time: 0.2514s, loss: 0.3401: 100%|██████████| 1080/1080 [06:03<00:00,  2.98it/s]\n",
      "  0%|          | 0/270 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Training time:363.0459s, loss: 0.3401, accuracy: 85.4144%, f1_score: 0.6889, auc: 0.8961\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 270/270 [00:21<00:00, 12.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Validation loss: 0.2700, accuracy: 88.7885%, f1_score: 0.7768, best_thres: 0.0000, auc: 0.9395\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/1080 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Start epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch num: 1080. Avg. batch proc. time: 0.2555s, loss: 0.2255: 100%|██████████| 1080/1080 [06:08<00:00,  2.93it/s]\n",
      "  0%|          | 0/270 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Training time:368.8642s, loss: 0.2255, accuracy: 90.8322%, f1_score: 0.8160, auc: 0.9571\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 270/270 [00:21<00:00, 12.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Validation loss: 0.2664, accuracy: 89.5993%, f1_score: 0.7865, best_thres: 0.0000, auc: 0.9451\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/1080 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Start epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch num: 1080. Avg. batch proc. time: 0.2547s, loss: 0.1547: 100%|██████████| 1080/1080 [06:07<00:00,  3.66it/s]\n",
      "  0%|          | 0/270 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Training time:368.0699s, loss: 0.1547, accuracy: 94.1478%, f1_score: 0.8841, auc: 0.9792\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 270/270 [00:21<00:00, 12.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Validation loss: 0.2801, accuracy: 90.4563%, f1_score: 0.8097, best_thres: 0.0000, auc: 0.9515\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/1080 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Start epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch num: 1080. Avg. batch proc. time: 0.2566s, loss: 0.1067: 100%|██████████| 1080/1080 [06:09<00:00,  3.71it/s]\n",
      "  0%|          | 0/270 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Training time:369.9736s, loss: 0.1067, accuracy: 96.1342%, f1_score: 0.9235, auc: 0.9900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 270/270 [00:21<00:00, 12.53it/s]\n",
      "  0%|          | 0/1080 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Validation loss: 0.3175, accuracy: 90.3289%, f1_score: 0.7961, best_thres: 0.0000, auc: 0.9471\n",
      "-> Start epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch num: 1080. Avg. batch proc. time: 0.2559s, loss: 0.0824: 100%|██████████| 1080/1080 [06:08<00:00,  3.74it/s]\n",
      "  0%|          | 0/270 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Training time:369.0720s, loss: 0.0824, accuracy: 97.0898%, f1_score: 0.9424, auc: 0.9940\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 270/270 [00:21<00:00, 12.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Validation loss: 0.3162, accuracy: 91.1513%, f1_score: 0.8154, best_thres: 0.0000, auc: 0.9494\n",
      "\t* Start 1 fold\n",
      "\t* Building dataset...\n",
      "\t* Building model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/270 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t* Building model time:2.5562s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 270/270 [00:22<00:00, 12.19it/s]\n",
      "  0%|          | 0/1080 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t* Validation loss before training: 0.6259, accuracy:73.1874, f1_score: 0.2879, best_thres: 0.0000, auc: 0.6201\n",
      "\n",
      " ==================== Training Bert model o device: cuda ====================\n",
      "-> Start epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch num: 1080. Avg. batch proc. time: 0.2546s, loss: 0.3327: 100%|██████████| 1080/1080 [06:07<00:00,  3.78it/s]\n",
      "  0%|          | 0/270 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Training time:367.8882s, loss: 0.3327, accuracy: 86.1064%, f1_score: 0.7050, auc: 0.9003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 270/270 [00:21<00:00, 12.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Validation loss: 0.2740, accuracy: 88.8812%, f1_score: 0.7744, best_thres: 0.0000, auc: 0.9349\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/1080 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Start epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch num: 1080. Avg. batch proc. time: 0.2563s, loss: 0.2243: 100%|██████████| 1080/1080 [06:10<00:00,  3.66it/s]\n",
      "  0%|          | 0/270 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Training time:370.4188s, loss: 0.2243, accuracy: 91.1218%, f1_score: 0.8212, auc: 0.9571\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 270/270 [00:21<00:00, 12.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Validation loss: 0.2720, accuracy: 89.6456%, f1_score: 0.7864, best_thres: 0.0000, auc: 0.9411\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/1080 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Start epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch num: 1080. Avg. batch proc. time: 0.2566s, loss: 0.1541: 100%|██████████| 1080/1080 [06:10<00:00,  3.61it/s]\n",
      "  0%|          | 0/270 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Training time:370.5147s, loss: 0.1541, accuracy: 94.2318%, f1_score: 0.8852, auc: 0.9793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 270/270 [00:21<00:00, 12.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Validation loss: 0.2737, accuracy: 90.0973%, f1_score: 0.7988, best_thres: 0.0000, auc: 0.9470\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/1080 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Start epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch num: 1080. Avg. batch proc. time: 0.2573s, loss: 0.1088: 100%|██████████| 1080/1080 [06:10<00:00,  3.57it/s]\n",
      "  0%|          | 0/270 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Training time:370.9790s, loss: 0.1088, accuracy: 96.1169%, f1_score: 0.9230, auc: 0.9891\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 270/270 [00:21<00:00, 12.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Validation loss: 0.2938, accuracy: 90.5722%, f1_score: 0.8059, best_thres: 0.0000, auc: 0.9485\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/1080 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Start epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch num: 1080. Avg. batch proc. time: 0.2571s, loss: 0.0841: 100%|██████████| 1080/1080 [06:10<00:00,  2.91it/s]\n",
      "  0%|          | 0/270 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Training time:370.8441s, loss: 0.0841, accuracy: 96.9508%, f1_score: 0.9395, auc: 0.9935\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 270/270 [00:22<00:00, 12.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Validation loss: 0.2967, accuracy: 91.2092%, f1_score: 0.8193, best_thres: 0.0000, auc: 0.9517\n",
      "\t* Start 2 fold\n",
      "\t* Building dataset...\n",
      "\t* Building model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/270 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t* Building model time:2.1128s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 270/270 [00:22<00:00, 12.09it/s]\n",
      "  0%|          | 0/1080 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t* Validation loss before training: 0.8161, accuracy:24.7047, f1_score: 0.3819, best_thres: 0.0000, auc: 0.3402\n",
      "\n",
      " ==================== Training Bert model o device: cuda ====================\n",
      "-> Start epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch num: 1080. Avg. batch proc. time: 0.2547s, loss: 0.3617: 100%|██████████| 1080/1080 [06:08<00:00,  3.68it/s]\n",
      "  0%|          | 0/270 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Training time:368.4080s, loss: 0.3617, accuracy: 84.7165%, f1_score: 0.6598, auc: 0.8783\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 270/270 [00:21<00:00, 12.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Validation loss: 0.2782, accuracy: 88.2326%, f1_score: 0.7533, best_thres: 0.0000, auc: 0.9340\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/1080 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Start epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch num: 1080. Avg. batch proc. time: 0.2545s, loss: 0.2436: 100%|██████████| 1080/1080 [06:07<00:00,  3.68it/s]\n",
      "  0%|          | 0/270 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Training time:367.9894s, loss: 0.2436, accuracy: 90.2849%, f1_score: 0.8008, auc: 0.9486\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 270/270 [00:21<00:00, 12.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Validation loss: 0.2581, accuracy: 89.6456%, f1_score: 0.7833, best_thres: 0.0000, auc: 0.9454\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/1080 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Start epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch num: 1080. Avg. batch proc. time: 0.2540s, loss: 0.1732: 100%|██████████| 1080/1080 [06:06<00:00,  3.76it/s]\n",
      "  0%|          | 0/270 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Training time:366.5799s, loss: 0.1732, accuracy: 93.3341%, f1_score: 0.8660, auc: 0.9741\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 270/270 [00:21<00:00, 12.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Validation loss: 0.2734, accuracy: 90.2131%, f1_score: 0.7972, best_thres: 0.0000, auc: 0.9468\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/1080 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Start epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch num: 1080. Avg. batch proc. time: 0.2545s, loss: 0.1220: 100%|██████████| 1080/1080 [06:07<00:00,  3.64it/s]\n",
      "  0%|          | 0/270 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Training time:367.2615s, loss: 0.1220, accuracy: 95.4885%, f1_score: 0.9099, auc: 0.9868\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 270/270 [00:21<00:00, 12.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Validation loss: 0.2738, accuracy: 90.5837%, f1_score: 0.8043, best_thres: 0.0000, auc: 0.9476\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/1080 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Start epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch num: 1080. Avg. batch proc. time: 0.2521s, loss: 0.0940: 100%|██████████| 1080/1080 [06:03<00:00,  3.74it/s]\n",
      "  0%|          | 0/270 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Training time:363.8840s, loss: 0.0940, accuracy: 96.5686%, f1_score: 0.9315, auc: 0.9918\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 270/270 [00:21<00:00, 12.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Validation loss: 0.2801, accuracy: 90.9775%, f1_score: 0.8189, best_thres: 0.0000, auc: 0.9483\n",
      "\t* Start 3 fold\n",
      "\t* Building dataset...\n",
      "\t* Building model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/270 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t* Building model time:2.3203s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 270/270 [00:22<00:00, 12.20it/s]\n",
      "  0%|          | 0/1080 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t* Validation loss before training: 0.9139, accuracy:25.2288, f1_score: 0.3945, best_thres: 0.0000, auc: 0.3313\n",
      "\n",
      " ==================== Training Bert model o device: cuda ====================\n",
      "-> Start epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch num: 1080. Avg. batch proc. time: 0.2504s, loss: 0.3365: 100%|██████████| 1080/1080 [06:01<00:00,  3.75it/s]\n",
      "  0%|          | 0/270 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Training time:362.0764s, loss: 0.3365, accuracy: 85.8347%, f1_score: 0.6960, auc: 0.8970\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 270/270 [00:21<00:00, 12.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Validation loss: 0.2714, accuracy: 88.5208%, f1_score: 0.7769, best_thres: 0.0000, auc: 0.9392\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/1080 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Start epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch num: 1080. Avg. batch proc. time: 0.2528s, loss: 0.2258: 100%|██████████| 1080/1080 [06:04<00:00,  3.57it/s]\n",
      "  0%|          | 0/270 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Training time:364.9195s, loss: 0.2258, accuracy: 91.0902%, f1_score: 0.8188, auc: 0.9560\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 270/270 [00:21<00:00, 12.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Validation loss: 0.2670, accuracy: 89.5749%, f1_score: 0.7928, best_thres: 0.0000, auc: 0.9467\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/1080 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Start epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch num: 1080. Avg. batch proc. time: 0.2511s, loss: 0.1542: 100%|██████████| 1080/1080 [06:02<00:00,  3.67it/s]\n",
      "  0%|          | 0/270 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Training time:362.3658s, loss: 0.1542, accuracy: 94.1567%, f1_score: 0.8828, auc: 0.9792\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 270/270 [00:21<00:00, 12.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Validation loss: 0.2718, accuracy: 90.6637%, f1_score: 0.8063, best_thres: 0.0000, auc: 0.9517\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/1080 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Start epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch num: 1080. Avg. batch proc. time: 0.2511s, loss: 0.1117: 100%|██████████| 1080/1080 [06:02<00:00,  3.76it/s]\n",
      "  0%|          | 0/270 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Training time:362.8305s, loss: 0.1117, accuracy: 95.9027%, f1_score: 0.9181, auc: 0.9887\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 270/270 [00:21<00:00, 12.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Validation loss: 0.2856, accuracy: 90.8375%, f1_score: 0.8150, best_thres: 0.0000, auc: 0.9528\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/1080 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Start epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch num: 1080. Avg. batch proc. time: 0.2526s, loss: 0.0857: 100%|██████████| 1080/1080 [06:03<00:00,  3.74it/s]\n",
      "  0%|          | 0/270 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Training time:363.8398s, loss: 0.0857, accuracy: 96.9364%, f1_score: 0.9387, auc: 0.9932\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 270/270 [00:21<00:00, 12.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Validation loss: 0.3133, accuracy: 90.8027%, f1_score: 0.8176, best_thres: 0.0000, auc: 0.9498\n",
      "\t* Start 4 fold\n",
      "\t* Building dataset...\n",
      "\t* Building model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/270 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t* Building model time:2.3788s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 270/270 [00:22<00:00, 12.20it/s]\n",
      "  0%|          | 0/1080 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t* Validation loss before training: 0.9451, accuracy:25.5647, f1_score: 0.4070, best_thres: 0.0000, auc: 0.5101\n",
      "\n",
      " ==================== Training Bert model o device: cuda ====================\n",
      "-> Start epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch num: 1080. Avg. batch proc. time: 0.2520s, loss: 0.3339: 100%|██████████| 1080/1080 [06:03<00:00,  3.81it/s]\n",
      "  0%|          | 0/270 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Training time:363.6945s, loss: 0.3339, accuracy: 85.7449%, f1_score: 0.6933, auc: 0.8984\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 270/270 [00:21<00:00, 12.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Validation loss: 0.2760, accuracy: 88.5903%, f1_score: 0.7719, best_thres: 0.0000, auc: 0.9365\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/1080 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Start epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch num: 1080. Avg. batch proc. time: 0.2510s, loss: 0.2251: 100%|██████████| 1080/1080 [06:02<00:00,  3.76it/s]\n",
      "  0%|          | 0/270 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Training time:362.7633s, loss: 0.2251, accuracy: 91.1597%, f1_score: 0.8195, auc: 0.9555\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 270/270 [00:21<00:00, 12.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Validation loss: 0.2746, accuracy: 89.5981%, f1_score: 0.7977, best_thres: 0.0000, auc: 0.9447\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/1080 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Start epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch num: 1080. Avg. batch proc. time: 0.2503s, loss: 0.1556: 100%|██████████| 1080/1080 [06:01<00:00,  3.60it/s]\n",
      "  0%|          | 0/270 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Training time:361.6226s, loss: 0.1556, accuracy: 94.1306%, f1_score: 0.8820, auc: 0.9783\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 270/270 [00:21<00:00, 12.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Validation loss: 0.2774, accuracy: 90.1541%, f1_score: 0.8156, best_thres: 0.0000, auc: 0.9504\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/1080 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Start epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch num: 1080. Avg. batch proc. time: 0.2526s, loss: 0.1110: 100%|██████████| 1080/1080 [06:03<00:00,  3.72it/s]\n",
      "  0%|          | 0/270 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Training time:363.7692s, loss: 0.1110, accuracy: 95.9288%, f1_score: 0.9183, auc: 0.9887\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 270/270 [00:21<00:00, 12.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Validation loss: 0.3022, accuracy: 90.3162%, f1_score: 0.8185, best_thres: 0.0000, auc: 0.9506\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/1080 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Start epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch num: 1080. Avg. batch proc. time: 0.2523s, loss: 0.0845: 100%|██████████| 1080/1080 [06:03<00:00,  3.70it/s]\n",
      "  0%|          | 0/270 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Training time:363.8187s, loss: 0.0845, accuracy: 97.1362%, f1_score: 0.9426, auc: 0.9931\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 270/270 [00:21<00:00, 12.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Validation loss: 0.3171, accuracy: 90.4436%, f1_score: 0.8194, best_thres: 0.0000, auc: 0.9512\n",
      "k_best_score : [0.95151878 0.95165389 0.94826516 0.9528116  0.95116603]\n",
      "k weights : [[[0.20009162]]\n",
      "\n",
      " [[0.20012003]]\n",
      "\n",
      " [[0.19940743]]\n",
      "\n",
      " [[0.20036348]]\n",
      "\n",
      " [[0.20001744]]]\n",
      "0.8169916434540391 0.53\n",
      "dev auc:  0.9500097555117696\n",
      "\t* Saving dev result...\n",
      "\t* Predicting...\n",
      "\t* Saving test result...\n"
     ]
    }
   ],
   "source": [
    "model_version = 'V6.5'     # 模型版本\n",
    "scheme_version = 'V6.5'     # 方案版本\n",
    "# train_df = pd.read_csv(train_all_path)\n",
    "train_df = pd.read_csv(train_augmented_V0204_path)\n",
    "# test_df = pd.read_csv(test_path)\n",
    "# train_df = pd.read_csv(train_V0_path)\n",
    "test_df = pd.read_csv(test_V0_path)\n",
    "k = 5\n",
    "bert_tokenizer = BertTokenizer.from_pretrained(os.path.join(pretrained_bert_path, 'vocab.txt'))\n",
    "output_path = os.path.join(root_path, 'output/'+scheme_version+'.txt')\n",
    "\n",
    "print(\"\\t* K fold training and validating...\")\n",
    "test_preds, k_test_probas, dev_probas, dev_labels, best_f1, best_thres = k_fold_cross_val(train_df, test_df, k, bert_tokenizer, \n",
    "                                                                                          best_model_path, output_path, model_version)\n",
    "dev_preds = (dev_probas[:, 1] > best_thres).type(torch.long)\n",
    "fpr, tpr, thresholds = roc_curve(dev_labels, dev_probas[:, 1], pos_label=1)\n",
    "dev_auc = auc(fpr, tpr)\n",
    "print('dev auc: ',dev_auc)\n",
    "\n",
    "print(\"\\t* Saving dev result...\")\n",
    "with open(os.path.join(root_path, 'report/'+scheme_version+'_'+'classification_report.txt'), 'w') as fp:\n",
    "    fp.write(classification_report(dev_labels, dev_preds))\n",
    "    fp.write('\\n')\n",
    "    fp.write('f1-score: {:.4f}'.format(f1_score(dev_labels, dev_preds)))\n",
    "    fp.write(' auc: {:.4f}'.format(dev_auc))\n",
    "\n",
    "print(\"\\t* Predicting...\")\n",
    "test_df['pred'] = test_preds.cpu().numpy()\n",
    "k_test_probas = k_test_probas.cpu().numpy()\n",
    "\n",
    "print(\"\\t* Saving test result...\")\n",
    "# 保存预测结果\n",
    "time_str = '' + time.strftime(\"%Y%m%d%H%M\", time.localtime())                                  \n",
    "test_df[['dialog_id', 'reply_id', 'pred']].to_csv(os.path.join(root_path,'submission/'+scheme_version+'_'+time_str+'.csv'),\n",
    "                                                  sep='\\t',\n",
    "                                                  index=0,\n",
    "                                                  header=0)\n",
    "# 保存K折预测概率结果\n",
    "k_test_probas_path = os.path.join(root_path, 'result/'+scheme_version+'_'+str(k)+'_test_probas.npz')\n",
    "if not os.path.exists(k_test_probas_path):\n",
    "    np.save(k_test_probas_path, k_test_probas)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "V2.0.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "customer_churn_analysis",
   "language": "python",
   "name": "customer_churn_analysis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
